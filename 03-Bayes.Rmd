# Bayesian Statistics

## Introduction
In the previous section, we used JAGS to generate data from a model given set values for the parameters. Statistical inference turns this problem around: we aim to learn about the values of the parameters based on a given set of data. We do this in Bayesian inference by constructing distributions that model our uncertainty in the values of the parameters conditional on the data. If the models are simple enough then we can describe these distributions mathematically. However, this is rarely possible in practice. Instead, we can use JAGS, or similar software, to simulate from the distributions of the parameters conditional on the observed data and then use the simulated values to summarize the shapes of the distributions. 

## Prior and Posterior Distributions

There is one final ingredient that needs to be defined before we can conduct inference using JAGS -- the prior distribution. The mathematics of probability implies that we must assign an initial distribution to each of the parameters in the model in order to construct its conditional distribution given the data. This distribution is called the prior distribution. It encodes our uncertainty about the parameter before data are observed (*a priori* in Latin). The conditional distribution that results from the combination of this distribution and the data is called the posterior distribution. It encodes our uncertainty about the parameter after the data are observed (*a posteriori*). 

There are different ways of constructing prior distributions that have different effects on the posterior. Some of the debate can be rather philosophical, and we will avoid this during this workshop. Instead, we will look at common prior distributions and the effects they have on inference.

### Example 3: Coin Tossing

Suppose that your friend has two coins, one of which is fair (i.e., it has a heads side and a tails side and lands heads side up 50% of the time) and one of which has heads on both sides (so it always lands heads side up). You close your eyes, pick one of the coins at random, hand it to your friend, and she tosses it. Your job is to decide whether you chose the fair code or the coin with two heads based on whether it landed heads or tails side up.

If the coin lands tails side up then it must be the fair coin. If the coin lands heads side up then it might either be the fair coin or the coin with two heads. It seems more likely that it would be the coin with two heads in this case -- since it has two heads and the fair coin only has one. In fact, you might guess that it is twice as likely to be the coin with two heads. 

How can we describe this mathematically?

If you pick the coin at random then there is a 50% chance that you choose either coin. This is the prior distribution. A priori, the probability that you chose the fair coin is .5 and the probability that you chose the coin with two heads is .5.  

If the coin lands tails side up then it must be the fair coin. In this case, the posterior distribution puts probability 1 on the fair coin and probability 0 on the coin with two heads (i.e., it must be the fair coin). 

If the coin lands heads side up then the posterior distribution assigns probability of 1/3 that it is the fair coin and probability of 2/3 that it is the coin with two heads.

What happens if your friend tosses the same coin a second time and it lands heads side up again? In this case, the posterior probability that it is the fair coin is 1/5 and the probability that it is the coin with two heads is 4/5. More generally, if your friend tosses the coin $n$ times and it lands heads side up every time then the posterior distribution assigns probability $1/2^k$ that you chose the fair coin and probability $1-1/2^k$ that you chose the coin with two heads. 

## Interpreting Posterior Distributions

In Bayesian inference, information about the parameters is contained in the posterior distribution. Consider the coin tossing problem. Initially, the prior distribution assigns equal probability to the fair coin and the coin with two heads. After the coin lands heads side up once, the posterior distribution assigns probability of $2/3$ to the coin with two heads. There is still a 1/3 chance that it is the fair coin, but it is more likely to be the coin with two heads. After the coin lands heads side up twice, the probability assigned to the coin with two heads increases to 4/5. As the coin is tossed more and more and lands heads side up more and more often you become more and more certain that you chose the coin with two heads. There is always a possibility that you picked the fair coin and it happened to land heads side up every time by chance, but this becomes less and less likely the more the coin is tossed. If the coin lands heads side up 10 times in a row then the posterior distribution assigns probability $1-1/2^10=.9990234$ to the coin with two heads. This is so unlikely to happen if the coin is fair, that the posterior distribution assigns a probability on only $.000976$ to the fair coin (less than 1 in 10,000). 

In most cases, the distributions we assign to parameters will be continuous not discrete. For example, prior distributions are often selected to be normal distributions. In these cases, we summarize information about the parameters using the basic statistics for describing the shape of a distribution that you have likelihood encountered in introductory statistics classes. These include the mean, median, and mode to identify the centre (location) of the distribution and the standard deviation to measure the spread of the distribution. 

A more informative summary of a distribution that measures both its location and its spread is an interval that contains a certain amount of the distribution's probability. An interval that contains $X$% of the probability under the posterior distribution for a parameter is called an $X$% credible interval. For example, the interval between the 2.5 percentile and 97.5 percentile contains 95% of the probability and hence forms a 95% credible interval. Credible intervals are the Bayesian analogue of confidence intervals in classical statistics.

These are the statistics that are provided in the output from the `summary()` function and in the caterpillar plot. 

In practice, it is common to provided one of the measures of location of the posterior distribution (most of the mean) as a point estimate (the best guess at the value of a parameter) and either the standard deviation or a credible interval as a measure of the spread. Bigger values of the standard deviation or wider credible intervals indicate that there is more uncertainty about the value of the parameter (the posterior distribution is more spread out). Smaller values of the standard deviation or narrower credible intervals indicate that there is less uncertainty about the value of the parameter (the posterior distribution is less spread out). 

### Example 4: Coin Tossing 2 (The Gory Math!)

Consider the coin tossing problem again, but suppose this time that your friend only has one coin which has both a heads and a tails side. However, it is not fair and lands heads side up with some probability between 0 and 1, which I'll call $p$, and tails side up with probability $1-p$. E.g., if $p=.1$ then it lands heads side up on 1 out of every 10 tosses, on average, and if $p=.9$ then it lands heads side up on 9 out of every 10 tosses, on average. Your job this time is guess the value of $p$. 

If you know nothing about $p$ to begin with then you might choose a prior distribution that assigns the same probability to every value between 0 and 1. This is the uniform distribution. Suppose that your friend tosses the coin $n$ times and it lands heads side up $k$ times. In this case, it's possible to compute the posterior distribution exactly. The plots below compare the posterior distribution for two different values of $n$, 5 and 10, and two different values of $k/n$, the proportion of heads. These are .2 (meaning that $k$ is equal to 1 if $n=5$ and 2 if $n=10$) and .8 (meaning that $k$ is equal to 4 if $n=5$ and $8$ if $k=10$). 
```{r, echo = FALSE, eval = TRUE}
mydata <- crossing(n = c(5,10),
                   r = c(.2,.8),
                   p = (1:100)/101) %>%
  mutate(k = r * n,
         d = dbeta(p, k + 1, n - k + 1))

mydata %>%
  ggplot(aes(x = p, y = d)) + 
  geom_line() + 
  facet_grid(n ~ r) +
  geom_vline(aes(xintercept = r), lty = 2) +
  ylab("Posterior Distribution")
  
```

In each case, the posterior distribution is highest around the value $k/n$, .2 on the left or .8 on the right, which is shown by the dotted line. This indicates that $k/n$ is the best guess at the probability of tossing a head given the observed data. However, the posterior distributions are more spread out in the top plots and more concentrated about the values in the bottom plots. This implies that we are more confident about the values in bottom when we have collected more data. 

This is also evident in the following table which provides the posterior means and the lower and upper bounds of the 95% credible intervals for $p$ for each combination of $n$ and $k$. The posterior mean only depends on the ratio $k/n$, but the confidence interval becomes narrower as $n$ increases. 
```{r}
mytab <- crossing(n = c(5,20),
                  r = c(.2,.8)) %>%
  mutate(k = n * r,
         Mean = r,
         Lower = qbeta(.025,k + 1, n - k + 1),
         Upper = qbeta(.975, k+1, n - k + 1))

mytab %>%
  select(-r) %>%
  kable(digits = 2)
```
For example, the posterior mean is equal to $.2$ both when $k=1$ and $n=5$ and when $k=4$ and $n=20$. However, the width of the 95% credible is .64-.04=.60 in the first case and .42-.08=.34 in the second. The secodn interval is just over half as wide, indicating that we are much more confident about the value of $p$. 

### Example 4 (continued): Coin Tossing 2 (Simulation with JAGS)

The number of heads in our coin toss example follows a binomial distribution with parameters $n$ and $p$. This is a very simple model, and the DAG looks like this

![DAG coin tossing model](Figures/coin_toss_dag.png){width=150px}

This binomial distribution is implemented with the function `dbinom(p,n)` in JAGS (yes, the `n` and the `p` are backwards). The uniform distribution over the interval from 0 to 1 is implemented with the function `dunif()`. Putting these together, we can implement the model for the coin tossin problem with the code
```{r, eval = FALSE, echo = TRUE}
model{
  ## Distribution of observed data
  k ~ dbinom(p,n)
  
  ## Prior distribution
  p ~ dunif(0,1)
}
```
This code is also provided in the file `Examples/Example_4/example_4.jags`. 

The following `R` code would compile this model and then generate 1000 samples from the distribution of `p` for the case in which the coin is tossed 10 times and lands heads side up 4 times. 
```{r, eval = FALSE, echo = TRUE}
## Define data
jags_data_4 <- list(n = 10, k = 4)

## Compile model
jags_model_4 <- jags.model("Examples/Example_4/example_4.jags", jags_data_4)

## Generate samples
jags_samples_4 <- coda.samples(jags_model_4, "p", n.iter = 1000)

## Compute summary statistics
summary(jags_samples_4)
```
Run the code and compare the mean and 95% credible interval from the sample with the theoretical mean and 95% credible interval given above. You should find that the results are close but not exact. 

### Exercise 3

1) Try running the code again and increase the number of sample drawn from the posterior distribution from 1000 to 5000 or 10000. You should find that the results become closer and closer to the theoretical value as the number of samples generated increases.  

2) Change the values of `n` and `k` and confirm that you can reproduce the values in the table above (with some uncertainty).
